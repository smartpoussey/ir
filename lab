import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.naive_bayes import MultinomialNB, BernoulliNB
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer

def load_data_from_csv(file_path):
    df = pd.read_csv(file_path)

    texts = df['text'].astype(str).tolist()
    labels = df['label'].tolist()
    return texts, labels

def train_and_evaluate(texts, labels, model_type='multinomial'):

    X_train, X_test, y_train, y_test = train_test_split(
        texts, labels, test_size=0.25, random_state=42)

    if model_type == 'multinomial':
        vectorizer = CountVectorizer()
        classifier = MultinomialNB()
    elif model_type == 'bernoulli':
        vectorizer = CountVectorizer(binary=True)
        classifier = BernoulliNB()

    X_train_counts = vectorizer.fit_transform(X_train)
    X_test_counts = vectorizer.transform(X_test)

    classifier.fit(X_train_counts, y_train)
    y_pred = classifier.predict(X_test_counts)
    accuracy = accuracy_score(y_test, y_pred)

    print(f"{model_type.capitalize()} Naïve Bayes Accuracy: {accuracy:.2f}")

    return vectorizer, classifier

def classify_text(text, vectorizer, classifier):
    text_counts = vectorizer.transform([text])
    prediction = classifier.predict(text_counts)[0]
    return prediction

file_path = 'data.csv'
texts, labels = load_data_from_csv(file_path)


vectorizer_m, classifier_m = train_and_evaluate(texts, labels, model_type='multinomial')
vectorizer_b, classifier_b = train_and_evaluate(texts, labels, model_type='bernoulli')

new_text = "I like using this product"

result_m = classify_text(new_text, vectorizer_m, classifier_m)
result_b = classify_text(new_text, vectorizer_b, classifier_b)

print(f"Multinomial Model Prediction: {result_m}")
print(f"Bernoulli Model Prediction: {result_b}")



import numpy as np
import pandas as pd

from numpy import dot
from numpy.linalg import norm

from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer


def load_data_from_csv(file_path):
    df = pd.read_csv(file_path)
    documents = df['text'].tolist()
    return documents

documents = load_data_from_csv('documents.csv')
# Text Preprocessing - Simple tokenization using split
filtered_token_set = []
for doc in documents:
    tokens = doc.lower().split()
    filtered_tokens = [token for token in tokens if token.isalnum()]
    filtered_token_set.append(" ".join(filtered_tokens))

# TF-IDF Vectorization
tv = TfidfVectorizer()
document_matrix = tv.fit_transform(filtered_token_set)
td = pd.DataFrame(document_matrix.toarray(), columns=tv.get_feature_names_out())

# Query Processing
query = ["natural language processing"]
query_vector = tv.transform(query).toarray()[0]

# Cosine Similarity Calculation
cos_sim = []
for i in range(td.shape[0]):
    doc_vector = td.iloc[i].values
    similarity = dot(doc_vector, query_vector) / (norm(doc_vector) * norm(query_vector))
    cos_sim.append(similarity)

# Print Results
print("Cosine Similarity Scores:")
for idx, score in enumerate(cos_sim):
    print(f"Document {idx+1}: {score:.4f}")

# Phase 1: Select top k documents
k = 2
top_k_indices = sorted(range(len(cos_sim)), key=lambda i: cos_sim[i], reverse=True)[:k]

relevant_documents = [i+1 for i in top_k_indices]
non_relevant_documents = [i+1 for i in range(len(documents)) if i not in top_k_indices]
print("\nRelevant docs in phase 1:", relevant_documents)
print("Non-relevant docs in phase 1:", non_relevant_documents)

# Phase 2: Binary Independence Model
cv = CountVectorizer()
cv_ans = cv.fit_transform(filtered_token_set)
tdm = pd.DataFrame(cv_ans.toarray(), columns=cv.get_feature_names_out())
tdm_bool = tdm.astype(bool).astype(int)

print("\nBinary Document-Term Matrix:")
print(tdm_bool)

# Apply Binary Independence Model (BIM) Formula for Query Terms
N = len(documents)  # Total number of documents
S = len(relevant_documents)  # Number of relevant documents

bim_scores = {}

# Compute BIM score only for query terms
query_tokens = query[0].split()
print("\nQuery tokens:", query_tokens)

for term in query_tokens:
    if term in tdm.columns:
        n = tdm_bool[term].sum()  # Number of documents containing the term
        s = tdm_bool.iloc[top_k_indices][term].sum()  # Number of relevant documents containing the term
        # Apply BIM formula 
        numerator = (S + 0.5) / (S - s + 0.5)
        denominator = (n - s + 0.5) / (N - S - n + s + 0.5)
        bim_score = np.log(numerator / denominator)
        bim_scores[term] = bim_score

# Print BIM Scores for Query Terms
print("\nBinary Independence Model (BIM) Scores for Query Terms:")
for term, score in bim_scores.items():
    print(f"{term}: {score:.4f}")

# Calculate final scores for each document
score_list = []
for j in range(tdm_bool.shape[0]):
    vec = tdm_bool.iloc[j]
    score = 0
    for term in query_tokens:
        if term in bim_scores:
            score += vec[term] * bim_scores[term]
    score_list.append(score)
print("\nFinal BIM scores:", score_list)



import pandas as pd
import numpy as np
import re
import math
from collections import Counter

df = pd.read_csv("/content/drive/MyDrive/documents.csv")

documents = df["text"].tolist()

query = "Machine learning Algorithms"

def tokenize(doc):
    return doc.lower().split()

def normalize(tokens):
    return [token[:-1] if token.endswith('s') and not token.endswith('ss') else token for token in tokens]

def compute_dictionary(normalized_docs):
    dictionary = set()
    for doc in normalized_docs:
        dictionary.update(doc)
    return sorted(list(dictionary))

def create_term_document_matrix(query_terms, normalized_docs, model_type="binary"):
    num_terms = len(query_terms)
    num_docs = len(normalized_docs)
    matrix = np.zeros((num_terms, num_docs), dtype=int)

    for i, term in enumerate(query_terms):
        for j, doc in enumerate(normalized_docs):
            if model_type == "binary":
                matrix[i, j] = 1 if term in doc else 0
            elif model_type == "multinomial":
                matrix[i, j] = doc.count(term)

    return matrix

def probabilistic_model(query_terms, term_doc_matrix, model_type="binary", relevant_docs=None):
    num_docs = term_doc_matrix.shape[1]
    relevant_docs = relevant_docs or []
    S = len(relevant_docs)

    t_i = np.sum(term_doc_matrix > 0, axis=1) if model_type == "binary" else np.sum(term_doc_matrix, axis=1)

    s_i = np.sum(term_doc_matrix[:, relevant_docs], axis=1) if S > 0 else np.zeros(len(query_terms))

    p_i = (s_i + 0.5) / (S + 1)
    u_i = (t_i - s_i + 0.5) / (num_docs - S + 1)

    weights = np.log((p_i * (1 - u_i)) / ((1 - p_i) * u_i))

    scores = np.dot(weights, term_doc_matrix)

    return scores

def main(model_type="binary"):
    tokenized_docs = [tokenize(doc) for doc in documents]
    normalized_docs = [normalize(tokens) for tokens in tokenized_docs]

    print(f"\nRunning {model_type.upper()} Model\n")

    tokenized_query = tokenize(query)
    normalized_query = normalize(tokenized_query)

    term_doc_matrix = create_term_document_matrix(normalized_query, normalized_docs, model_type)

    initial_scores = probabilistic_model(normalized_query, term_doc_matrix, model_type)
    ranked_indices = np.argsort(-initial_scores)

    print("Initial ranking:")
    for rank, idx in enumerate(ranked_indices, 1):
        print(f"Rank {rank}: d{idx+1} (Score: {initial_scores[idx]:.4f})")
    print()

    top_2 = ranked_indices[:2]
    relevant_docs = list(top_2)

    print(f"Assuming documents {['d' + str(i + 1) for i in relevant_docs]} are relevant")

    updated_scores = probabilistic_model(normalized_query, term_doc_matrix, model_type, relevant_docs)
    final_ranked_indices = np.argsort(-updated_scores)

    print("Final ranking after relevance feedback:")
    for rank, idx in enumerate(final_ranked_indices, 1):
        print(f"Rank {rank}: d{idx+1} (Score: {updated_scores[idx]:.4f})")

if _name_ == "_main_":
    main(model_type="binary")
    main(model_type="multinomial")



from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import BernoulliNB, MultinomialNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Sample dataset
texts = ["Free money now!!!", "Call me tomorrow", "Win a lottery prize now", 
         "Meeting scheduled at 3PM", "Congratulations, you won a gift"]
labels = [1, 0, 1, 0, 1]  # 1: Spam, 0: Not Spam

# Convert text to feature vectors
vectorizer = CountVectorizer(binary=True)  # Use binary=True for BernoulliNB
X = vectorizer.fit_transform(texts)

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.3, random_state=42)

# Bernoulli Naïve Bayes
bnb = BernoulliNB()
bnb.fit(X_train, y_train)
y_pred_bnb = bnb.predict(X_test)
print("BernoulliNB Accuracy:", accuracy_score(y_test, y_pred_bnb))

# Multinomial Naïve Bayes (uses word counts)
vectorizer = CountVectorizer(binary=False)  # Use word counts for MultinomialNB
X = vectorizer.fit_transform(texts)
X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.3, random_state=42)

mnb = MultinomialNB()
mnb.fit(X_train, y_train)
y_pred_mnb = mnb.predict(X_test)
print("MultinomialNB Accuracy:", accuracy_score(y_test, y_pred_mnb))

I got this for text classification
